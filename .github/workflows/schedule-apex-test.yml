name: Salesforce Test Runner

on:
  pull_request:
    types: [opened, synchronize, reopened]
  schedule:
    # Every Monday at 11:00 PM EST (4:00 AM UTC Tuesday)
    - cron: '0 4 * * 2'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test Level'
        required: false
        default: 'RunLocalTests'
        type: choice
        options:
          - RunLocalTests
          - RunAllTestsInOrg
          - RunSpecifiedTests

jobs:
  run-salesforce-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Salesforce CLI
        run: |
          npm install -g @salesforce/cli
          sf version

      - name: Authenticate to Salesforce
        run: |
          echo "${{ secrets.SF_AUTH_URL }}" > auth_url.txt
          sf org login sfdx-url --sfdx-url-file auth_url.txt --alias target-org --set-default-dev-hub --set-default
          rm -f auth_url.txt

      - name: Check for Active Deployments
        id: check_deployment
        run: |
          echo "Checking for active deployments..."
          MAX_WAIT=1800  # 30 minutes max wait time
          WAIT_TIME=0
          POLL_INTERVAL=30  # Check every 30 seconds
          
          while [ $WAIT_TIME -lt $MAX_WAIT ]; do
            # Query for in-progress deployments
            ACTIVE_DEPLOYMENTS=$(sf data query --query "SELECT Id, Status, CreatedBy.Name, StartDate FROM DeployRequest WHERE Status IN ('Pending', 'InProgress', 'Queued') ORDER BY StartDate DESC LIMIT 1" --target-org target-org --json || echo '{"result":{"records":[]}}')
            
            DEPLOYMENT_COUNT=$(echo "$ACTIVE_DEPLOYMENTS" | jq '.result.records | length')
            
            if [ "$DEPLOYMENT_COUNT" -eq "0" ]; then
              echo "‚úÖ No active deployments found. Proceeding with tests..."
              echo "deployment_wait_time=$WAIT_TIME" >> $GITHUB_OUTPUT
              break
            else
              DEPLOYMENT_ID=$(echo "$ACTIVE_DEPLOYMENTS" | jq -r '.result.records[0].Id')
              DEPLOYMENT_STATUS=$(echo "$ACTIVE_DEPLOYMENTS" | jq -r '.result.records[0].Status')
              DEPLOYMENT_USER=$(echo "$ACTIVE_DEPLOYMENTS" | jq -r '.result.records[0].CreatedBy.Name')
              
              echo "‚è≥ Active deployment found (ID: $DEPLOYMENT_ID, Status: $DEPLOYMENT_STATUS, User: $DEPLOYMENT_USER)"
              echo "Waiting $POLL_INTERVAL seconds before checking again..."
              sleep $POLL_INTERVAL
              WAIT_TIME=$((WAIT_TIME + POLL_INTERVAL))
            fi
          done
          
          if [ $WAIT_TIME -ge $MAX_WAIT ]; then
            echo "‚ö†Ô∏è Maximum wait time reached. Deployment still in progress."
            echo "deployment_timeout=true" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Run Tests
        id: run_tests
        run: |
          echo "Running Salesforce tests..."
          TEST_LEVEL="${{ github.event.inputs.test_level || 'RunLocalTests' }}"
          
          # Run tests and capture output
          sf apex run test --test-level $TEST_LEVEL --code-coverage --result-format json --wait 60 --target-org target-org > test_results.json || true
          
          # Parse results
          cat test_results.json
          
          # Extract key metrics
          STATUS=$(jq -r '.result.summary.outcome' test_results.json)
          TOTAL_TESTS=$(jq -r '.result.summary.testsRan' test_results.json)
          PASSED=$(jq -r '.result.summary.passing' test_results.json)
          FAILED=$(jq -r '.result.summary.failing' test_results.json)
          SKIPPED=$(jq -r '.result.summary.skipped' test_results.json)
          CODE_COVERAGE=$(jq -r '.result.summary.testRunCoverage' test_results.json)
          ORG_COVERAGE=$(jq -r '.result.summary.orgWideCoverage' test_results.json)
          EXEC_TIME=$(jq -r '.result.summary.testExecutionTimeInMs' test_results.json)
          
          # Convert execution time to seconds
          EXEC_TIME_SEC=$(echo "scale=2; $EXEC_TIME / 1000" | bc)
          
          # Get failing test classes
          FAILING_TESTS=$(jq -r '.result.tests[] | select(.Outcome == "Fail") | "\(.MethodName) - \(.Message)"' test_results.json | head -10)
          FAILING_CLASSES=$(jq -r '.result.tests[] | select(.Outcome == "Fail") | .ApexClass.Name' test_results.json | sort -u | tr '\n' ', ' | sed 's/,$//')
          
          # Export variables
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "test_level=$TEST_LEVEL" >> $GITHUB_OUTPUT
          echo "passed=$PASSED" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT
          echo "skipped=$SKIPPED" >> $GITHUB_OUTPUT
          echo "code_coverage=$CODE_COVERAGE" >> $GITHUB_OUTPUT
          echo "org_coverage=$ORG_COVERAGE" >> $GITHUB_OUTPUT
          echo "exec_time=$EXEC_TIME_SEC" >> $GITHUB_OUTPUT
          
          # Save failing tests and classes to file
          echo "$FAILING_TESTS" > failing_tests.txt
          echo "$FAILING_CLASSES" > failing_classes.txt
          
          # Set exit code based on status
          if [ "$STATUS" = "Failed" ]; then
            exit 1
          fi

      - name: Analyze Code Coverage Per Class
        id: coverage_analysis
        if: always()
        run: |
          echo "Analyzing individual class code coverage..."
          COVERAGE_THRESHOLD=95
          
          # Extract individual class coverage
          jq -r '.result.coverage.coverage[] | select(.name != null) | "\(.name)|\(.coveredPercent // 0)"' test_results.json > class_coverage.txt
          
          # Find classes below 95% coverage
          LOW_COVERAGE_CLASSES=""
          LOW_COVERAGE_COUNT=0
          
          while IFS='|' read -r CLASS_NAME COVERAGE_PCT; do
            # Convert coverage to integer for comparison (remove % if present)
            COVERAGE_INT=$(echo "$COVERAGE_PCT" | sed 's/%//' | cut -d'.' -f1)
            
            if [ "$COVERAGE_INT" -lt "$COVERAGE_THRESHOLD" ]; then
              LOW_COVERAGE_CLASSES="${LOW_COVERAGE_CLASSES}${CLASS_NAME}: ${COVERAGE_PCT}%\n"
              LOW_COVERAGE_COUNT=$((LOW_COVERAGE_COUNT + 1))
            fi
          done < class_coverage.txt
          
          # Count total classes
          TOTAL_CLASSES=$(wc -l < class_coverage.txt | tr -d ' ')
          
          # Save low coverage classes to file
          echo -e "$LOW_COVERAGE_CLASSES" > low_coverage_classes.txt
          
          # Export variables
          echo "low_coverage_count=$LOW_COVERAGE_COUNT" >> $GITHUB_OUTPUT
          echo "total_classes=$TOTAL_CLASSES" >> $GITHUB_OUTPUT
          echo "coverage_threshold=$COVERAGE_THRESHOLD" >> $GITHUB_OUTPUT
          
          # Create formatted coverage report
          cat << EOF > coverage_report.md
          ## üìä Code Coverage Analysis (Threshold: ${COVERAGE_THRESHOLD}%)
          
          **Total Classes:** $TOTAL_CLASSES
          **Classes Below ${COVERAGE_THRESHOLD}%:** $LOW_COVERAGE_COUNT
          
          EOF
          
          if [ "$LOW_COVERAGE_COUNT" -gt "0" ]; then
            echo "### ‚ö†Ô∏è Classes Below ${COVERAGE_THRESHOLD}% Coverage" >> coverage_report.md
            echo '```' >> coverage_report.md
            cat low_coverage_classes.txt >> coverage_report.md
            echo '```' >> coverage_report.md
          else
            echo "‚úÖ All classes meet the ${COVERAGE_THRESHOLD}% coverage threshold!" >> coverage_report.md
          fi
          
          # Add top 10 classes with full coverage report
          echo "" >> coverage_report.md
          echo "### üìã Individual Class Coverage" >> coverage_report.md
          echo '```' >> coverage_report.md
          sort -t'|' -k2 -n class_coverage.txt | head -20 | while IFS='|' read -r CLASS_NAME COVERAGE_PCT; do
            echo "${CLASS_NAME}: ${COVERAGE_PCT}%" >> coverage_report.md
          done
          echo '```' >> coverage_report.md
          
          cat coverage_report.md

      - name: Format Test Results for Slack
        id: format_slack
        if: always()
        run: |
          FAILING_CLASSES=$(cat failing_classes.txt || echo "None")
          # Remove trailing comma if present and limit length
          FAILING_CLASSES=$(echo "$FAILING_CLASSES" | sed 's/,$//')
          echo "failing_classes=$FAILING_CLASSES" >> $GITHUB_OUTPUT

      - name: Format Test Results for GitHub
        id: format_github
        if: always()
        run: |
          STATUS="${{ steps.run_tests.outputs.status }}"
          
          # Determine emoji based on status
          if [ "$STATUS" = "Passed" ]; then
            EMOJI="‚úÖ"
            COLOR="success"
          else
            EMOJI="‚ùå"
            COLOR="failure"
          fi
          
          # Read failing tests
          FAILING_TESTS=$(cat failing_tests.txt || echo "None")
          
          # Create formatted output
          cat << EOF > github_comment.md
          ## ${EMOJI} Salesforce Test Results
          
          | Metric | Value |
          |--------|-------|
          | **Status** | \`$STATUS\` |
          | **Test Level** | \`${{ steps.run_tests.outputs.test_level }}\` |
          | **Total Tests** | ${{ steps.run_tests.outputs.total_tests }} |
          | **Passed** | ‚úÖ ${{ steps.run_tests.outputs.passed }} |
          | **Failed** | ‚ùå ${{ steps.run_tests.outputs.failed }} |
          | **Skipped** | ‚è≠Ô∏è ${{ steps.run_tests.outputs.skipped }} |
          | **Code Coverage** | ${{ steps.run_tests.outputs.code_coverage }}% |
          | **Org Coverage** | ${{ steps.run_tests.outputs.org_coverage }}% |
          | **Execution Time** | ${{ steps.run_tests.outputs.exec_time }}s |
          | **Deployment Wait Time** | ${{ steps.check_deployment.outputs.deployment_wait_time || 0 }}s |
          
          EOF
          
          if [ "${{ steps.run_tests.outputs.failed }}" != "0" ]; then
            cat << EOF >> github_comment.md
          ### ‚ö†Ô∏è Failing Tests
          \`\`\`
          $FAILING_TESTS
          \`\`\`
          EOF
          fi
          
          # Add coverage analysis
          if [ -f coverage_report.md ]; then
            cat coverage_report.md >> github_comment.md
          fi
          
          cat github_comment.md

      - name: Comment on PR
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const comment = fs.readFileSync('github_comment.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Prepare MS Teams Notification
        id: prepare_teams_notification
        if: always()
        run: |
          # Read low coverage classes
          LOW_COVERAGE_CLASSES=$(cat low_coverage_classes.txt || echo "None")
          LOW_COVERAGE_CLASSES=$(echo "$LOW_COVERAGE_CLASSES" | head -10)
          
          # Determine color based on status
          if [ "${{ steps.run_tests.outputs.status }}" = "Passed" ] && [ "${{ steps.coverage_analysis.outputs.low_coverage_count }}" = "0" ]; then
            THEME_COLOR="00FF00"  # Green
          elif [ "${{ steps.run_tests.outputs.status }}" = "Passed" ]; then
            THEME_COLOR="FFA500"  # Orange (passed but coverage issues)
          else
            THEME_COLOR="FF0000"  # Red
          fi
          
          # Create MS Teams webhook payload
          cat << 'EOF' > teams_payload.json
          {
            "@type": "MessageCard",
            "@context": "https://schema.org/extensions",
            "summary": "Salesforce Test Results",
            "themeColor": "THEME_COLOR_PLACEHOLDER",
            "title": "STATUS_EMOJI Salesforce Test Results",
            "sections": [
              {
                "activityTitle": "Test Execution Summary",
                "facts": [
                  {
                    "name": "Status:",
                    "value": "TEST_STATUS"
                  },
                  {
                    "name": "Test Level:",
                    "value": "TEST_LEVEL"
                  },
                  {
                    "name": "Total Tests:",
                    "value": "TOTAL_TESTS"
                  },
                  {
                    "name": "Passed:",
                    "value": "PASSED_TESTS"
                  },
                  {
                    "name": "Failed:",
                    "value": "FAILED_TESTS"
                  },
                  {
                    "name": "Code Coverage:",
                    "value": "CODE_COVERAGE%"
                  },
                  {
                    "name": "Org Coverage:",
                    "value": "ORG_COVERAGE%"
                  },
                  {
                    "name": "Execution Time:",
                    "value": "EXEC_TIME seconds"
                  }
                ]
              },
              {
                "activityTitle": "Code Coverage Analysis",
                "facts": [
                  {
                    "name": "Coverage Threshold:",
                    "value": "COVERAGE_THRESHOLD%"
                  },
                  {
                    "name": "Total Classes:",
                    "value": "TOTAL_CLASSES"
                  },
                  {
                    "name": "Classes Below Threshold:",
                    "value": "LOW_COVERAGE_COUNT"
                  }
                ],
                "text": "LOW_COVERAGE_TEXT"
              },
              {
                "activityTitle": "Repository Information",
                "facts": [
                  {
                    "name": "Repository:",
                    "value": "REPO_NAME"
                  },
                  {
                    "name": "Branch:",
                    "value": "BRANCH_NAME"
                  },
                  {
                    "name": "Triggered by:",
                    "value": "ACTOR_NAME"
                  }
                ]
              }
            ],
            "potentialAction": [
              {
                "@type": "OpenUri",
                "name": "View Workflow Run",
                "targets": [
                  {
                    "os": "default",
                    "uri": "WORKFLOW_URL"
                  }
                ]
              },
              {
                "@type": "OpenUri",
                "name": "View Repository",
                "targets": [
                  {
                    "os": "default",
                    "uri": "REPO_URL"
                  }
                ]
              }
            ]
          }
          EOF
          
          # Replace placeholders
          STATUS_EMOJI="${{ steps.run_tests.outputs.status == 'Passed' && '‚úÖ' || '‚ùå' }}"
          
          # Escape and format low coverage classes for Teams
          if [ "${{ steps.coverage_analysis.outputs.low_coverage_count }}" != "0" ]; then
            LOW_COVERAGE_TEXT="‚ö†Ô∏è Classes needing attention:\n\n${LOW_COVERAGE_CLASSES}"
          else
            LOW_COVERAGE_TEXT="‚úÖ All classes meet the coverage threshold!"
          fi
          
          sed -i.bak "s/THEME_COLOR_PLACEHOLDER/$THEME_COLOR/g" teams_payload.json
          sed -i.bak "s/STATUS_EMOJI/$STATUS_EMOJI/g" teams_payload.json
          sed -i.bak "s/TEST_STATUS/${{ steps.run_tests.outputs.status }}/g" teams_payload.json
          sed -i.bak "s/TEST_LEVEL/${{ steps.run_tests.outputs.test_level }}/g" teams_payload.json
          sed -i.bak "s/TOTAL_TESTS/${{ steps.run_tests.outputs.total_tests }}/g" teams_payload.json
          sed -i.bak "s/PASSED_TESTS/${{ steps.run_tests.outputs.passed }}/g" teams_payload.json
          sed -i.bak "s/FAILED_TESTS/${{ steps.run_tests.outputs.failed }}/g" teams_payload.json
          sed -i.bak "s/CODE_COVERAGE/${{ steps.run_tests.outputs.code_coverage }}/g" teams_payload.json
          sed -i.bak "s/ORG_COVERAGE/${{ steps.run_tests.outputs.org_coverage }}/g" teams_payload.json
          sed -i.bak "s/EXEC_TIME/${{ steps.run_tests.outputs.exec_time }}/g" teams_payload.json
          sed -i.bak "s/COVERAGE_THRESHOLD/${{ steps.coverage_analysis.outputs.coverage_threshold }}/g" teams_payload.json
          sed -i.bak "s/TOTAL_CLASSES/${{ steps.coverage_analysis.outputs.total_classes }}/g" teams_payload.json
          sed -i.bak "s/LOW_COVERAGE_COUNT/${{ steps.coverage_analysis.outputs.low_coverage_count }}/g" teams_payload.json
          sed -i.bak "s|LOW_COVERAGE_TEXT|${LOW_COVERAGE_TEXT}|g" teams_payload.json
          sed -i.bak "s|REPO_NAME|${{ github.repository }}|g" teams_payload.json
          sed -i.bak "s|BRANCH_NAME|${{ github.head_ref || github.ref_name }}|g" teams_payload.json
          sed -i.bak "s|ACTOR_NAME|${{ github.actor }}|g" teams_payload.json
          sed -i.bak "s|WORKFLOW_URL|${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|g" teams_payload.json
          sed -i.bak "s|REPO_URL|${{ github.server_url }}/${{ github.repository }}|g" teams_payload.json
          
          cat teams_payload.json

      - name: Send MS Teams Notification
        if: always()
        run: |
          if [ -n "${{ secrets.MSTEAMS_WEBHOOK }}" ]; then
            curl -H "Content-Type: application/json" \
                 -d @teams_payload.json \
                 "${{ secrets.MSTEAMS_WEBHOOK }}"
            echo "‚úÖ MS Teams notification sent successfully"
          else
            echo "‚ö†Ô∏è MSTEAMS_WEBHOOK secret not configured. Skipping MS Teams notification."
          fi

      - name: Send Slack Notification
        if: always()
        uses: slackapi/slack-github-action@v1.24.0
        with:
          payload: |
            {
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "${{ steps.run_tests.outputs.status == 'Passed' && '‚úÖ' || '‚ùå' }} Salesforce Test Results"
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Status:*\n`${{ steps.run_tests.outputs.status }}`"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Test Level:*\n`${{ steps.run_tests.outputs.test_level }}`"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Total Tests:*\n${{ steps.run_tests.outputs.total_tests }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Passed:*\n‚úÖ ${{ steps.run_tests.outputs.passed }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Failed:*\n‚ùå ${{ steps.run_tests.outputs.failed }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Skipped:*\n‚è≠Ô∏è ${{ steps.run_tests.outputs.skipped }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Code Coverage:*\n${{ steps.run_tests.outputs.code_coverage }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Org Coverage:*\n${{ steps.run_tests.outputs.org_coverage }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Execution Time:*\n${{ steps.run_tests.outputs.exec_time }}s"
                    }
                  ]
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Coverage Threshold:*\n${{ steps.coverage_analysis.outputs.coverage_threshold }}%"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Total Classes:*\n${{ steps.coverage_analysis.outputs.total_classes }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Classes Below ${{ steps.coverage_analysis.outputs.coverage_threshold }}%:*\n${{ steps.coverage_analysis.outputs.low_coverage_count }}"
                    }
                  ]
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Repository:* ${{ github.repository }}\n*Branch:* `${{ github.head_ref || github.ref_name }}`\n*Triggered by:* ${{ github.actor }}"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Failed Classes:*\n${{ steps.run_tests.outputs.failed != '0' && format('`{0}`', steps.format_slack.outputs.failing_classes) || 'None' }}"
                  }
                },
                {
                  "type": "actions",
                  "elements": [
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "View Workflow Run"
                      },
                      "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                    },
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "üì• Download Test Results"
                      },
                      "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}#artifacts"
                    }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: salesforce-test-results
          path: |
            test_results.json
            class_coverage.txt
            low_coverage_classes.txt
            coverage_report.md
            github_comment.md

      - name: Fail workflow if tests failed
        if: steps.run_tests.outputs.status == 'Failed'
        run: exit 1