name: Salesforce Scheduled Tests with Coverage Report

on:
  schedule:
    # Run every Monday at 8:00 AM UTC (adjust timezone as needed)
    - cron: '0 8 * * 1'
  workflow_dispatch:
    inputs:
      sandbox_environment:
        description: 'Salesforce sandbox environment to test'
        required: true
        default: 'TEST2'
        type: choice
        options:
          - TEST2
          - DEV
          - QA
          - UAT
      test_level:
        description: 'Test level to run'
        required: true
        default: 'RunLocalTests'
        type: choice
        options:
          - RunLocalTests
          - RunAllTestsInOrg
          - RunSpecifiedTests
      send_slack_notification:
        description: 'Send Slack notification'
        required: true
        default: true
        type: boolean

env:
  SFDX_USE_GENERIC_UNIX_KEYCHAIN: true
  SFDX_DOMAIN_RETRY: 300
  SFDX_DISABLE_APP_HUB: true
  SFDX_LOG_LEVEL: DEBUG

jobs:
  salesforce-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    strategy:
      matrix:
        # Define multiple sandbox environments for scheduled runs
        sandbox: 
          - name: "TEST2"
            alias: "TEST2"
          - name: "DEV" 
            alias: "DEV"
        # Only run on specified environment for manual triggers
        include:
          - sandbox:
              name: ${{ github.event.inputs.sandbox_environment || 'TEST2' }}
              alias: ${{ github.event.inputs.sandbox_environment || 'TEST2' }}
      # For manual runs, only test the specified environment
      exclude:
        - ${{ github.event_name == 'workflow_dispatch' && matrix.sandbox.name != github.event.inputs.sandbox_environment }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install Dependencies
        run: |
          npm ci
          npm install -g @salesforce/cli

      - name: Verify Salesforce CLI Installation
        run: |
          sf --version
          sf plugins --core

      - name: Setup Python for Report Generation
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Python Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 jinja2

      - name: Authenticate to Salesforce Sandbox
        env:
          SALESFORCE_JWT_SECRET_KEY: ${{ secrets[format('SALESFORCE_JWT_SECRET_KEY_{0}', matrix.sandbox.alias)] }}
          SALESFORCE_CONSUMER_KEY: ${{ secrets[format('SALESFORCE_CONSUMER_KEY_{0}', matrix.sandbox.alias)] }}
          SALESFORCE_USERNAME: ${{ secrets[format('SALESFORCE_USERNAME_{0}', matrix.sandbox.alias)] }}
          SALESFORCE_INSTANCE_URL: ${{ secrets[format('SALESFORCE_INSTANCE_URL_{0}', matrix.sandbox.alias)] }}
        run: |
          # Create JWT key file
          echo "$SALESFORCE_JWT_SECRET_KEY" > server.key
          
          # Authenticate using JWT
          sf org login jwt \
            --client-id "$SALESFORCE_CONSUMER_KEY" \
            --jwt-key-file server.key \
            --username "$SALESFORCE_USERNAME" \
            --instance-url "$SALESFORCE_INSTANCE_URL" \
            --alias "${{ matrix.sandbox.alias }}"
          
          # Verify authentication
          sf org display --target-org "${{ matrix.sandbox.alias }}"
          
          # Clean up key file
          rm server.key

      - name: Create Test Reports Directory
        run: |
          TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
          REPORT_DIR="./test-coverage-reports/${TIMESTAMP}_${{ matrix.sandbox.alias }}"
          mkdir -p "$REPORT_DIR"
          echo "REPORT_DIR=$REPORT_DIR" >> $GITHUB_ENV
          echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_ENV

      - name: Run Salesforce Tests
        id: run_tests
        env:
          TEST_LEVEL: ${{ github.event.inputs.test_level || 'RunLocalTests' }}
        run: |
          echo "Starting Salesforce tests for ${{ matrix.sandbox.alias }} sandbox..."
          echo "Test Level: $TEST_LEVEL"
          
          # Get org info
          sf org display --target-org "${{ matrix.sandbox.alias }}" --json > "$REPORT_DIR/org_info.json"
          
          # Run tests with timeout and proper error handling
          set +e  # Don't exit on error, we want to capture results
          
          TEST_OUTPUT=$(sf apex run test \
            --target-org "${{ matrix.sandbox.alias }}" \
            --test-level "$TEST_LEVEL" \
            --code-coverage \
            --result-format json \
            --wait 60 \
            --output-dir "$REPORT_DIR" \
            --json 2>&1)
          
          TEST_EXIT_CODE=$?
          echo "TEST_EXIT_CODE=$TEST_EXIT_CODE" >> $GITHUB_ENV
          
          # Save raw output
          echo "$TEST_OUTPUT" > "$REPORT_DIR/raw_test_output.json"
          
          # Extract test run ID if available
          TEST_RUN_ID=$(echo "$TEST_OUTPUT" | jq -r '.result.testRunId // empty' 2>/dev/null || echo "")
          
          if [ -n "$TEST_RUN_ID" ] && [ "$TEST_RUN_ID" != "null" ]; then
            echo "TEST_RUN_ID=$TEST_RUN_ID" >> $GITHUB_ENV
            echo "Test Run ID: $TEST_RUN_ID"
            
            # Get detailed test results
            sf apex get test \
              --target-org "${{ matrix.sandbox.alias }}" \
              --test-run-id "$TEST_RUN_ID" \
              --code-coverage \
              --result-format json \
              --output-dir "$REPORT_DIR" || true
          else
            echo "Failed to get test run ID or tests failed to start"
            echo "Raw output: $TEST_OUTPUT"
          fi
          
          set -e  # Re-enable exit on error

      - name: Generate Coverage Reports
        if: always()
        run: |
          # Create enhanced report generation script
          cat > "$REPORT_DIR/generate_enhanced_report.py" << 'EOF'
          #!/usr/bin/env python3
          import json
          import os
          import sys
          from datetime import datetime
          import glob
          
          def load_json_file(filepath):
              """Load JSON file safely"""
              try:
                  with open(filepath, 'r') as f:
                      return json.load(f)
              except Exception as e:
                  print(f"Error loading {filepath}: {e}")
                  return None
          
          def find_test_result_file(report_dir):
              """Find the test result JSON file"""
              patterns = [
                  'test-result-*.json',
                  'test-result.json',
                  'raw_test_output.json'
              ]
              
              for pattern in patterns:
                  files = glob.glob(os.path.join(report_dir, pattern))
                  if files:
                      return files[0]
              return None
          
          def generate_enhanced_report(report_dir, sandbox_name, timestamp):
              """Generate comprehensive coverage report with GitHub Actions integration"""
              
              # Load test results
              test_result_file = find_test_result_file(report_dir)
              if not test_result_file:
                  print("No test result file found")
                  # Create a basic report structure
                  data = {"coverage": {}, "tests": [], "summary": {"outcome": "No Results"}}
              else:
                  data = load_json_file(test_result_file)
                  if not data:
                      data = {"coverage": {}, "tests": [], "summary": {"outcome": "Failed to Load"}}
              
              # Handle different data structures
              if 'result' in data:
                  data = data['result']
              
              # Extract coverage and test data
              coverage_data = data.get('coverage', {})
              test_results = data.get('tests', [])
              summary = data.get('summary', {})
              
              # Calculate metrics
              total_classes = len(coverage_data)
              covered_classes = sum(1 for cls in coverage_data.values() if cls.get('coveredPercent', 0) > 0)
              avg_coverage = sum(cls.get('coveredPercent', 0) for cls in coverage_data.values()) / total_classes if total_classes > 0 else 0
              
              passed_tests = sum(1 for test in test_results if test.get('Outcome') == 'Pass')
              failed_tests = sum(1 for test in test_results if test.get('Outcome') == 'Fail')
              total_tests = len(test_results)
              
              # Generate summary for Slack
              slack_summary = {
                  "sandbox": sandbox_name,
                  "timestamp": timestamp,
                  "total_tests": total_tests,
                  "passed_tests": passed_tests,
                  "failed_tests": failed_tests,
                  "total_classes": total_classes,
                  "avg_coverage": round(avg_coverage, 2),
                  "status": "‚úÖ PASSED" if failed_tests == 0 and total_tests > 0 else "‚ùå FAILED",
                  "coverage_status": "‚úÖ Good" if avg_coverage >= 75 else "‚ö†Ô∏è Needs Improvement" if avg_coverage >= 50 else "‚ùå Critical"
              }
              
              # Save Slack summary
              with open(os.path.join(report_dir, 'slack_summary.json'), 'w') as f:
                  json.dump(slack_summary, f, indent=2)
              
              # Generate detailed HTML report
              html_content = f"""
          <!DOCTYPE html>
          <html>
          <head>
              <title>Salesforce Test Coverage Report - {sandbox_name}</title>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1.0">
              <style>
                  body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }}
                  .container {{ max-width: 1200px; margin: 0 auto; background-color: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                  .header {{ background: linear-gradient(135deg, #1589ee, #0066cc); color: white; padding: 30px; border-radius: 8px 8px 0 0; }}
                  .header h1 {{ margin: 0; font-size: 2.5em; }}
                  .header p {{ margin: 10px 0 0 0; opacity: 0.9; }}
                  .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; padding: 30px; }}
                  .metric-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid #1589ee; }}
                  .metric-card h3 {{ margin: 0 0 10px 0; color: #333; }}
                  .metric-card .value {{ font-size: 2em; font-weight: bold; color: #1589ee; }}
                  .content {{ padding: 0 30px 30px 30px; }}
                  .coverage-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }}
                  .coverage-table th {{ background: #1589ee; color: white; padding: 15px; text-align: left; }}
                  .coverage-table td {{ padding: 12px 15px; border-bottom: 1px solid #eee; }}
                  .coverage-table tr:hover {{ background-color: #f8f9fa; }}
                  .low-coverage {{ background-color: #ffebee !important; }}
                  .medium-coverage {{ background-color: #fff3e0 !important; }}
                  .high-coverage {{ background-color: #e8f5e8 !important; }}
                  .failed-test {{ background-color: #ffcdd2 !important; }}
                  .status-badge {{ padding: 5px 10px; border-radius: 15px; font-size: 0.8em; font-weight: bold; }}
                  .status-critical {{ background: #ffcdd2; color: #c62828; }}
                  .status-warning {{ background: #fff3e0; color: #f57c00; }}
                  .status-good {{ background: #c8e6c9; color: #2e7d32; }}
                  .recommendations {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); padding: 25px; margin: 20px 0; border-radius: 8px; }}
                  .github-actions {{ background: #f6f8fa; border: 1px solid #d0d7de; padding: 20px; border-radius: 6px; margin: 20px 0; }}
                  .github-actions h3 {{ margin-top: 0; color: #24292f; }}
              </style>
          </head>
          <body>
              <div class="container">
                  <div class="header">
                      <h1>üöÄ Salesforce Test Coverage Report</h1>
                      <p>Sandbox: <strong>{sandbox_name}</strong> | Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')} | Run ID: {timestamp}</p>
                  </div>
                  
                  <div class="summary">
                      <div class="metric-card">
                          <h3>üìä Test Results</h3>
                          <div class="value">{passed_tests}/{total_tests}</div>
                          <p>Tests Passed</p>
                      </div>
                      <div class="metric-card">
                          <h3>üìà Code Coverage</h3>
                          <div class="value">{avg_coverage:.1f}%</div>
                          <p>Average Coverage</p>
                      </div>
                      <div class="metric-card">
                          <h3>üìÅ Classes Analyzed</h3>
                          <div class="value">{total_classes}</div>
                          <p>Total Classes</p>
                      </div>
                      <div class="metric-card">
                          <h3>‚ö†Ô∏è Failed Tests</h3>
                          <div class="value" style="color: {('#dc3545' if failed_tests > 0 else '#28a745')}">{failed_tests}</div>
                          <p>Need Attention</p>
                      </div>
                  </div>
                  
                  <div class="content">
          """
              
              if total_classes > 0:
                  # Coverage details table
                  html_content += """
                      <h2>üìã Class Coverage Details</h2>
                      <table class="coverage-table">
                          <thead>
                              <tr>
                                  <th>Class Name</th>
                                  <th>Coverage %</th>
                                  <th>Covered Lines</th>
                                  <th>Total Lines</th>
                                  <th>Status</th>
                                  <th>Uncovered Lines (Sample)</th>
                              </tr>
                          </thead>
                          <tbody>
                  """
                  
                  # Sort classes by coverage percentage
                  sorted_classes = sorted(coverage_data.items(), key=lambda x: x[1].get('coveredPercent', 0))
                  
                  for class_name, coverage in sorted_classes:
                      coverage_pct = coverage.get('coveredPercent', 0)
                      covered_lines = len(coverage.get('coveredLines', []))
                      uncovered_lines = coverage.get('uncoveredLines', [])
                      total_lines = covered_lines + len(uncovered_lines)
                      
                      # Determine coverage class and status
                      if coverage_pct < 50:
                          css_class = "low-coverage"
                          status = '<span class="status-badge status-critical">‚ùå Critical</span>'
                      elif coverage_pct < 75:
                          css_class = "medium-coverage"
                          status = '<span class="status-badge status-warning">‚ö†Ô∏è Needs Work</span>'
                      else:
                          css_class = "high-coverage"
                          status = '<span class="status-badge status-good">‚úÖ Good</span>'
                      
                      uncovered_lines_str = ', '.join(map(str, uncovered_lines[:5]))
                      if len(uncovered_lines) > 5:
                          uncovered_lines_str += f" ... (+{len(uncovered_lines) - 5} more)"
                      
                      html_content += f"""
                              <tr class="{css_class}">
                                  <td><strong>{class_name}</strong></td>
                                  <td><strong>{coverage_pct:.1f}%</strong></td>
                                  <td>{covered_lines}</td>
                                  <td>{total_lines}</td>
                                  <td>{status}</td>
                                  <td><code>{uncovered_lines_str}</code></td>
                              </tr>
                      """
                  
                  html_content += """
                          </tbody>
                      </table>
                  """
              
              # Failed tests section
              failed_test_list = [test for test in test_results if test.get('Outcome') == 'Fail']
              if failed_test_list:
                  html_content += """
                      <h2>‚ùå Failed Tests</h2>
                      <table class="coverage-table">
                          <thead>
                              <tr>
                                  <th>Test Class</th>
                                  <th>Test Method</th>
                                  <th>Error Message</th>
                                  <th>Stack Trace</th>
                              </tr>
                          </thead>
                          <tbody>
                  """
                  for test in failed_test_list:
                      class_name = test.get('ApexClass', {}).get('Name', 'Unknown') if isinstance(test.get('ApexClass'), dict) else str(test.get('ApexClass', 'Unknown'))
                      method_name = test.get('MethodName', 'Unknown')
                      message = test.get('Message', 'No message available')
                      stack_trace = test.get('StackTrace', 'No stack trace available')
                      
                      html_content += f"""
                              <tr class="failed-test">
                                  <td><strong>{class_name}</strong></td>
                                  <td><code>{method_name}</code></td>
                                  <td>{message}</td>
                                  <td><pre style="font-size: 0.8em; max-width: 300px; overflow-x: auto;">{stack_trace}</pre></td>
                              </tr>
                      """
                  html_content += """
                          </tbody>
                      </table>
                  """
              
              # GitHub Actions integration info
              html_content += f"""
                  <div class="github-actions">
                      <h3>üîß GitHub Actions Integration</h3>
                      <p><strong>Workflow:</strong> Salesforce Scheduled Tests</p>
                      <p><strong>Trigger:</strong> Scheduled (Monday 8:00 AM UTC) or Manual</p>
                      <p><strong>Environment:</strong> {sandbox_name}</p>
                      <p><strong>Report Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}</p>
                  </div>
              """
              
              # Recommendations section
              low_coverage_classes = [name for name, cov in coverage_data.items() if cov.get('coveredPercent', 0) < 75]
              critical_classes = [name for name, cov in coverage_data.items() if cov.get('coveredPercent', 0) < 50]
              
              html_content += f"""
                  <div class="recommendations">
                      <h2>üí° Recommendations for Improvement</h2>
                      
                      <h3>üéØ Priority Actions:</h3>
                      <ul>
              """
              
              if failed_tests > 0:
                  html_content += f"<li><strong>üî• URGENT:</strong> Fix {failed_tests} failing tests - These block deployment</li>"
              
              if len(critical_classes) > 0:
                  html_content += f"<li><strong>‚ö†Ô∏è HIGH:</strong> Address {len(critical_classes)} classes with &lt;50% coverage</li>"
              
              if len(low_coverage_classes) > 0:
                  html_content += f"<li><strong>üìà MEDIUM:</strong> Improve {len(low_coverage_classes)} classes with &lt;75% coverage</li>"
              
              if avg_coverage < 75:
                  html_content += f"<li><strong>üìä OVERALL:</strong> Current coverage is {avg_coverage:.1f}% - Target 75%+ for production</li>"
              
              html_content += """
                      </ul>
                      
                      <h3>üöÄ Best Practices:</h3>
                      <ul>
                          <li>Write comprehensive unit tests covering positive, negative, and edge cases</li>
                          <li>Use Test.startTest() and Test.stopTest() for governor limit testing</li>
                          <li>Implement @TestSetup methods for efficient test data creation</li>
                          <li>Target 85%+ coverage for critical business logic</li>
                          <li>Regularly run tests in CI/CD pipeline</li>
                      </ul>
                  </div>
              """
              
              html_content += """
                  </div>
              </div>
          </body>
          </html>
              """
              
              # Write HTML report
              with open(os.path.join(report_dir, 'coverage_report.html'), 'w') as f:
                  f.write(html_content)
              
              # Generate CSV report
              csv_content = "Class Name,Coverage %,Covered Lines,Total Lines,Status,Uncovered Lines Count\n"
              for class_name, coverage in sorted(coverage_data.items(), key=lambda x: x[1].get('coveredPercent', 0)):
                  coverage_pct = coverage.get('coveredPercent', 0)
                  covered_lines = len(coverage.get('coveredLines', []))
                  uncovered_lines_count = len(coverage.get('uncoveredLines', []))
                  total_lines = covered_lines + uncovered_lines_count
                  
                  if coverage_pct < 50:
                      status = "Critical"
                  elif coverage_pct < 75:
                      status = "Needs Improvement"
                  else:
                      status = "Good"
                  
                  csv_content += f'"{class_name}",{coverage_pct:.2f},{covered_lines},{total_lines},{status},{uncovered_lines_count}\n'
              
              with open(os.path.join(report_dir, 'coverage_report.csv'), 'w') as f:
                  f.write(csv_content)
              
              print(f"‚úÖ Enhanced reports generated successfully:")
              print(f"   üìÑ HTML Report: coverage_report.html")
              print(f"   üìä CSV Report: coverage_report.csv")
              print(f"   üí¨ Slack Summary: slack_summary.json")
              
              return slack_summary
          
          if __name__ == "__main__":
              if len(sys.argv) != 4:
                  print("Usage: python3 generate_enhanced_report.py <report_directory> <sandbox_name> <timestamp>")
                  sys.exit(1)
              
              generate_enhanced_report(sys.argv[1], sys.argv[2], sys.argv[3])
          EOF
          
          # Generate the enhanced report
          python3 "$REPORT_DIR/generate_enhanced_report.py" "$REPORT_DIR" "${{ matrix.sandbox.alias }}" "$TIMESTAMP"

      - name: Upload Test Results as Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: salesforce-test-results-${{ matrix.sandbox.alias }}-${{ env.TIMESTAMP }}
          path: |
            ${{ env.REPORT_DIR }}/**/*
          retention-days: 30

      - name: Set Job Status
        if: always()
        run: |
          if [ "$TEST_EXIT_CODE" -eq 0 ]; then
            echo "JOB_STATUS=success" >> $GITHUB_ENV
            echo "JOB_EMOJI=‚úÖ" >> $GITHUB_ENV
          else
            echo "JOB_STATUS=failure" >> $GITHUB_ENV
            echo "JOB_EMOJI=‚ùå" >> $GITHUB_ENV
          fi

      - name: Send Slack Notification
        if: always() && (github.event.inputs.send_slack_notification != 'false')
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
          SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL || '#salesforce-ci' }}
        run: |
          # Load Slack summary if it exists
          if [ -f "$REPORT_DIR/slack_summary.json" ]; then
            SLACK_DATA=$(cat "$REPORT_DIR/slack_summary.json")
            TOTAL_TESTS=$(echo "$SLACK_DATA" | jq -r '.total_tests')
            PASSED_TESTS=$(echo "$SLACK_DATA" | jq -r '.passed_tests')
            FAILED_TESTS=$(echo "$SLACK_DATA" | jq -r '.failed_tests')
            AVG_COVERAGE=$(echo "$SLACK_DATA" | jq -r '.avg_coverage')
            COVERAGE_STATUS=$(echo "$SLACK_DATA" | jq -r '.coverage_status')
          else
            TOTAL_TESTS="Unknown"
            PASSED_TESTS="Unknown"
            FAILED_TESTS="Unknown"
            AVG_COVERAGE="Unknown"
            COVERAGE_STATUS="‚ùì Unknown"
          fi
          
          # Determine overall status
          if [ "$JOB_STATUS" = "success" ] && [ "$FAILED_TESTS" = "0" ]; then
            OVERALL_STATUS="‚úÖ SUCCESS"
            COLOR="#36a64f"
          else
            OVERALL_STATUS="‚ùå FAILED"
            COLOR="#ff0000"
          fi
          
          # Create Slack message payload
          cat > slack_payload.json << EOF
          {
            "channel": "$SLACK_CHANNEL",
            "username": "Salesforce CI Bot",
            "icon_emoji": ":salesforce:",
            "attachments": [
              {
                "color": "$COLOR",
                "title": "$JOB_EMOJI Salesforce Test Results - ${{ matrix.sandbox.alias }}",
                "title_link": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
                "fields": [
                  {
                    "title": "Environment",
                    "value": "${{ matrix.sandbox.alias }}",
                    "short": true
                  },
                  {
                    "title": "Status",
                    "value": "$OVERALL_STATUS",
                    "short": true
                  },
                  {
                    "title": "Tests",
                    "value": "$PASSED_TESTS/$TOTAL_TESTS passed",
                    "short": true
                  },
                  {
                    "title": "Coverage",
                    "value": "$AVG_COVERAGE% $COVERAGE_STATUS",
                    "short": true
                  },
                  {
                    "title": "Failed Tests",
                    "value": "$FAILED_TESTS",
                    "short": true
                  },
                  {
                    "title": "Triggered By",
                    "value": "${{ github.event_name == 'schedule' && 'Scheduled Run' || github.actor }}",
                    "short": true
                  }
                ],
                "footer": "GitHub Actions",
                "footer_icon": "https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png",
                "ts": $(date +%s)
              }
            ]
          }
          EOF
          
          # Send to Slack using webhook
          if [ -n "$SLACK_WEBHOOK_URL" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data @slack_payload.json \
              "$SLACK_WEBHOOK_URL"
            echo "‚úÖ Slack notification sent via webhook"
          elif [ -n "$SLACK_BOT_TOKEN" ]; then
            curl -X POST -H "Authorization: Bearer $SLACK_BOT_TOKEN" \
              -H 'Content-type: application/json' \
              --data @slack_payload.json \
              https://slack.com/api/chat.postMessage
            echo "‚úÖ Slack notification sent via bot token"
          else
            echo "‚ö†Ô∏è No Slack credentials configured - skipping notification"
          fi

      - name: Comment on PR (if applicable)
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Try to read the slack summary
            let summary = {
              total_tests: 'Unknown',
              passed_tests: 'Unknown', 
              failed_tests: 'Unknown',
              avg_coverage: 'Unknown'
            };
            
            try {
              const summaryPath = path.join(process.env.REPORT_DIR, 'slack_summary.json');
              if (fs.existsSync(summaryPath)) {
                summary = JSON.parse(fs.readFileSync(summaryPath, 'utf8'));
              }
            } catch (error) {
              console.log('Could not read summary file:', error.message);
            }
            
            const status = process.env.JOB_STATUS === 'success' ? '‚úÖ PASSED' : '‚ùå FAILED';
            const emoji = process.env.JOB_EMOJI;
            
            const comment = `## ${emoji} Salesforce Test Results - ${{ matrix.sandbox.alias }}
            
            | Metric | Value |
            |--------|-------|
            | **Status** | ${status} |
            | **Tests Passed** | ${summary.passed_tests}/${summary.total_tests} |
            | **Failed Tests** | ${summary.failed_tests} |
            | **Code Coverage** | ${summary.avg_coverage}% |
            | **Environment** | ${{ matrix.sandbox.alias }} |
            
            üìä [View Detailed Report](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            ---
            *Automated by GitHub Actions*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Fail Job if Tests Failed
        if: env.TEST_EXIT_CODE != '0'
        run: |
          echo "‚ùå Tests failed with exit code: $TEST_EXIT_CODE"
          exit 1

  summary:
    runs-on: ubuntu-latest
    needs: salesforce-tests
    if: always()
    steps:
      - name: Generate Summary Report
        run: |
          echo "## üìä Salesforce Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Environment | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-------------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          # This would need to be enhanced to collect results from matrix jobs
          echo "| All Environments | ${{ needs.salesforce-tests.result }} | See individual job results |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üîó [View Detailed Results](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
